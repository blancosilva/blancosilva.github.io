---
layout: post
title: Testing.
date: 2016-09-10 
category: post
current: true
comments: true
---

```python
# Basic libraries to perform the core computations and handling of data
from numpy import float64, eye, zeros, float64, exp, unique
from pandas import read_csv
from math import fsum                         # Stable sums
from scipy.linalg import expm, expm2, expm3   # Different ways to compute matrix exponentials
from scipy.sparse.linalg import eigs, eigsh   # Eigenvectors/Eigenvalues

# These libraries are required to perform clustering and dimension reduction using the usual suspects
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.manifold import Isomap
from sklearn.decomposition import PCA
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.cluster import KMeans
from sklearn.manifold import SpectralEmbedding

# These libraries are required for interactive plotting
import matplotlib.pyplot as plt
%matplotlib inline
from bokeh.plotting import figure, ColumnDataSource
from bokeh.io import show  #push_notebook, output_notebook
from bokeh.models import HoverTool
from bokeh.models.widgets import Panel, Tabs
from bokeh.palettes import Blues9, Greens9, Reds9, Oranges9
```


```python
data = read_csv("Nutrition data.csv").dropna()

names = data['Shrt_Desc']

data.drop(['NDB_No', 'Shrt_Desc'], axis=1, inplace=True)
data = (data - data.mean())/data.std()
```

I usually like to start with the MeanShift clustering algorithm (provided my data contains less than 10,000 points, which is the case).  This gives me a first guess to the number of clusters that I might have.


```python
bandwidth = estimate_bandwidth(data.values, n_samples=8000)
MS_model = MeanShift(bandwidth=bandwidth, bin_seeding=True)
MS_model.fit(data.values)

clusters = len(unique(MS_model.labels_))

print "According to MeanShift, the data seems to cluster in %s groups" % clusters
```

    According to MeanShift, the data seems to cluster in 34 groups


I proceed to compute dimension reduction of the data, so I can present the information nicely later on.  I usually perform these four projections:
+ Isometric Map
+ Principal Component Analysis (PCA)
+ Spectral Embedding
+ Local Linear Embedding


```python
IM_model = Isomap().fit(data)
isomapped_df = IM_model.transform(data.values)

SpEmb_model = SpectralEmbedding().fit(data)
embedded_df = SpEmb_model.embedding_

PCA_model = PCA(n_components=2)
PCA_model.fit(data)
PCA(copy=True, n_components=2, whiten=False)
projected_df = PCA_model.transform(data)

LLE_model = LocallyLinearEmbedding().fit(data)
lle_df = LLE_model.transform(data)
```

I like to compare all clusterings to k-means.


```python
km_model = KMeans(n_jobs=-1,n_clusters=clusters).fit(data)
```

And that's it!  The next snippet of code builds an interactive diagram with the computed results.  It should open on another tab of your browser


```python
palette = Blues9 + Greens9 + Oranges9 + Reds9 + ["black"]

def source(model_1, model_2):
    return ColumnDataSource(
        data = dict(
            x = model_1[:,0],
            y = model_1[:,1],
            name = names,
            color = [palette[x] for x in model_2],
            cluster = model_2,
            size = [sum(model_2==x) for x in model_2]
        ) 
    )

tooltips = [("Name", "@name"), ("Cluster", "@cluster"), ("Cluster size", "@size")]

hover1 = HoverTool(tooltips=tooltips)
p1 = figure(plot_width=700, plot_height=700, title=None, toolbar_location="right")
p1.circle('x','y', color='color', source=source(embedded_df, km_model.labels_), size=10)
p1.add_tools(hover1)

hover2 = HoverTool(tooltips=tooltips)
p2 = figure(plot_width=700, plot_height=700, title=None, toolbar_location="right")
p2.circle('x','y', color='color', source=source(projected_df, km_model.labels_), size=10)
p2.add_tools(hover2)

hover3 = HoverTool(tooltips=tooltips)
p3 = figure(plot_width=700, plot_height=700, title=None, toolbar_location="right")
p3.circle('x','y', color='color', source=source(isomapped_df, km_model.labels_), size=10)
p3.add_tools(hover3)

hover4 = HoverTool(tooltips=tooltips)
p4 = figure(plot_width=700, plot_height=700, title=None, toolbar_location="right")
p4.circle('x','y', color='color', source=source(lle_df, km_model.labels_), size=10)
p4.add_tools(hover4)

tab1 = Panel(child=p1, title="Spectral Embedding")
tab2 = Panel(child=p2, title="PCA")
tab3 = Panel(child=p3, title="Isometric Map")
tab4 = Panel(child=p4, title="Locally Linear Embedding")

tabs = Tabs(tabs=[ tab1, tab2, tab3, tab4 ])

show(tabs)
```

Let us apply Joe's technique on each cluster computed by the k-means


```python
def cluster(label):
    return data[km_model.labels_==label]

def products(label):
    size = sum(km_model.labels_==label)
    products = names[km_model.labels_==label]
    new_indices = dict( zip(products.index, range(size)))
    return products.rename(new_indices)

def accumulate(i,j):
    return exp(-1.0 * fsum([(T[i,k] - T[j,k])**2 for k in range(T.shape[1])]))
    #return -1.0 * fsum([(T[i,k] - T[j,k])**2 for k in range(T.shape[1])]) instead ?

def M(a,q=4):
    # Computes a matrix exponential using Taylor's formula of degree q
    return expm3(a*C, q=q)
```

Construct a matrix $T$, with the numerical values of `cluster` as a `numpy.array`.  Covariance-like operation gives the matrix $C$, to which we change the diagonal---so that all columns add to zero.

Compute approximation of exponentials of $a \times C$ using Taylor for for small values of $a$.  Find eigenvalue/eigenvector convos for these matrices


```python
T = cluster(3).values

C = zeros((T.shape[0], T.shape[0]), dtype=float64)

for i in range(T.shape[0]):
    for j in range(T.shape[0]):
        C[i,j] = accumulate(i,j)
        
for i in range(T.shape[0]): C[i,i] = 0.0
    
diagonal = [fsum([C[i,j] for i in range(T.shape[0])]) for j in range(T.shape[0])]

for i in range(T.shape[0]): C[i,i] = diagonal[i]*(-1.0)
```


```python
Vs, vs = eigsh(M(1.0,10), k=5, which='LM')
```


```python
plt.plot(abs(vs[:,4]))
```




    [<matplotlib.lines.Line2D at 0x120515310>]




![png](expJoe_files/expJoe_15_1.png)



```python
products(3)[[n for n in range(len(vs[:,4])) if abs(vs[n,4])>0.1]]
```




    6                 BABYFOOD,JUC TREATS,FRUIT MEDLEY,TODD
    91                                 CANDIES,BUTTERSCOTCH
    106          CANDIES,FUDGE,VANILLA,PREPARED-FROM-RECIPE
    110                                  CANDIES,JELLYBEANS
    114                                CANDIES,MARSHMALLOWS
    150                                  JAMS AND PRESERVES
    151                                             JELLIES
    202    SWEETENERS,TABLETOP,SACCHARIN (SODIUM SACCHARIN)
    Name: Shrt_Desc, dtype: object




```python
def analyze_cluster(label, epsilon=1.0, degree=10, eigenvalues=5):
    
    def accumulate(i,j): return exp(-1.0 * fsum([(T[i,k] - T[j,k])**2 for k in range(T.shape[1])]))

    # Step #1: Construction of T
    T = cluster(label).values
    # Step #2: Construction of C
    C = zeros((T.shape[0], T.shape[0]), dtype=float64)
    for i in range(T.shape[0]):
        for j in range(T.shape[0]):
            C[i,j] = accumulate(i,j)
        
    for i in range(T.shape[0]): C[i,i] = 0.0
    
    diagonal = [fsum([C[i,j] for i in range(T.shape[0])]) for j in range(T.shape[0])]
    
    for i in range(T.shape[0]): C[i,i] = diagonal[i]*(-1.0)
        
    # Step #3: Construction of M = exp(epsilon * C) (using a Taylor's approximation 
    #          of the exponential with the required degree, and posterior computation 
    #          of the required number of eigenvalues
    Vs, vs = eigsh(expm3(epsilon*C), k=eigenvalues, which='LM')
    
    for step in range(eigenvalues):
        print "Eigenvalue %3f:\n" % Vs[step]
        print products(label)[[n for n in range(len(vs[:,step])) if abs(vs[n,step])>0.1]].values
        print 80*"*"
    
    plt.figure(figsize=(70, eigenvalues*20))
    for step in range(eigenvalues):
        plt.subplot(eigenvalues, 1, step)
        plt.plot(vs[:,step])
        plt.title("Visual inspection of $\lvert v \rvert for the eigenvector of the %s eigenvalue" % str(step))
        
    plt.show()
    
    
```


```python
analyze_cluster(24, epsilon=0.5)
```

    Eigenvalue -15603538542013924.000000:
    
    ['BEEF,LOIN,TOP LOIN STEAK,BNLS,LIP OFF,LN & FAT,0" FAT,SEL,RW'
     'BEEF,LOIN,TOP LOIN STK,BNLS,LIP OFF,LN,0" FAT,ALL GRDS,RAW'
     'BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP OFF,LN,0" FAT,SEL,RAW'
     'BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP-ON,LN,1/8" FAT,SEL,RAW'
     'Beef, ln, top ln stk, bneless lip-on, 1/8" fat all grdes raw']
    ********************************************************************************
    Eigenvalue -13513828670729106.000000:
    
    ['BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP OFF,LN,0" FAT,SEL,RAW'
     'BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP-ON,LN,1/8" FAT,SEL,RAW']
    ********************************************************************************
    Eigenvalue -12946981131763688.000000:
    
    ['BEEF,LOIN,TOP LOIN STK,BNLS,LIP OFF,LN,0" FAT,ALL GRDS,RAW'
     'BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP OFF,LN,0" FAT,SEL,RAW'
     'BEEF,LOIN,TOP LOIN STEAK,BNLESS,LIP-ON,LN,1/8" FAT,SEL,RAW'
     'Beef, ln, top ln stk, bneless lip-on, 1/8" fat all grdes raw']
    ********************************************************************************
    Eigenvalue -12309238206801948.000000:
    
    ['BEEF,LOIN,TOP LOIN STK,BNLS,LIP OFF,LN,0" FAT,ALL GRDS,RAW'
     'Beef, ln, top ln stk, bneless lip-on, 1/8" fat all grdes raw']
    ********************************************************************************
    Eigenvalue -11298641768012558.000000:
    
    ['BEEF,GROUND,93% LN MEAT / 7% FAT,PATTY,CKD,BRLD'
     'BEEF,GROUND,93% LN MEAT /7% FAT,PATTY,CKD,PAN-BROILED']
    ********************************************************************************



![png](expJoe_files/expJoe_18_1.png)



```python

```
