---
layout: post
title: MATH 524 Fall 17 Chapter 3
date: 2017-09-06 08:32
comments: true
category: forum
current: true
---

## Basic, Intermediate and CAS problems

<div class="alert alert-info">
The following is the list of <strong>non-advanced</strong> problems for Chapter 2 of the class notes.  There is a forum open at the end, so you can ask questions.  It is a great way to interact with the instructor and with other students in your class, should you need some assistance with any question. Please, <strong>do not post solutions</strong>.
</div>

1. The following sequences all converge to zero. 
	$$ \begin{align*}
	v_n &= n^{-10} &w_n &= 10^{-n} &x_n &=10^{-n^2} &y_n &=n^{10}3^{-n} &z_n &=10^{-3\cdot 2^n}
	\end{align*} $$
	Indicate the type of convergence (See Appendix \ref{appendix:convergence}).  

\begin{problem}[Advanced]\cite[p.249 \#4]{gautschi2011numerical}
Give an example of a positive sequence $\{ \varepsilon_n \}_{n\in\field{N}}$ converging to zero in such a way that $\lim_n \frac{\varepsilon_{n+1}}{\varepsilon_n^p} = 0$ for some $p>1$, but not converging to zero with any order $q > p$.
\end{problem}

\begin{problem}[Basic]
Find an example of a function $f\colon \field{R} \to \field{R}$ (different from the function in Example \ref{example:NewtonRaphsonloop}) with a unique root at $x=0$ for which the Newton-Raphson sequence is a loop no matter the initial guess $x_0\neq 0$: $x_{2n}=x_0$, $x_{2n+1}=-x_0$ for all $n \in \field{N}$.  Bonus points is your function is trigonometric.
\end{problem}

\begin{problem}[Intermediate]\cite[p.251 \#14]{gautschi2011numerical}
Consider the equation 
\begin{equation*}
x = \cos x.
\end{equation*}
\begin{enumerate}
	\item Show graphically that there exists a unique positive root $x^\star$.  Indicate approximately where it is located.
	\item Show that Newton's method applied to $f(x) = x-\cos x$ converges for any initial guess $x_0 \in \big[0,\frac{\pi}{2}\big]$.
\end{enumerate}
\end{problem}

\begin{problem}[Intermediate]\cite[p.251 \#16]{gautschi2011numerical}
Consider the equation 
\begin{equation*}
\tan x + \lambda x = 0, \quad (0 < \lambda < 1).
\end{equation*}
\begin{enumerate}
	\item Show graphically, as simply as possible, that there is exactly one root $x^\star$ in the interval $\big[ \frac{1}{2}\pi, \pi\big]$.
	\item Does Newton's method converge to the root $x^\star \in \big[ \frac{1}{2}\pi, \pi\big]$ if the initial approximation is taken to be $x_0 = \pi$?  Justify your answer.
\end{enumerate}
\end{problem}

\begin{problem}[Intermediate]\cite[p.252 \#17]{gautschi2011numerical}
Consider the equation 
\begin{equation*}
\log^2 x - x -1 = 0, (x > 0).
\end{equation*} 
\begin{enumerate}
	\item Graphical considerations suggest that there is exactly one positive solution $x^\star$, and that $0 <x^\star < 1$.  Prove this.
	\item What is the largest positive $0<x_0\leq 1$ such that Newton's method with $f(x) = \log^2 x - x -1 $ started at $x_0$ converges to $x^\star$?
\end{enumerate}
\end{problem}

\begin{problem}[Advanced]\cite[p.252 \#18]{gautschi2011numerical}
Consider Kepler's equation
\begin{equation*}
x -a \sin x - b = 0, \quad 0 <\abs{a}<1, \quad b \in \field{R}
\end{equation*}
where $a, b$ are parameters.
\begin{enumerate}
	\item Show that for each $a,b$ there is exactly one real solution $x^\star = x^\star(a,b)$ that satisfies
	\begin{equation*}
	b-\abs{a} \leq x^\star(a,b) \leq b+\abs{a}
	\end{equation*} 
	\item Let $m \in \field{N}$ satisfy $m\pi < b < (m+1)\pi$.  Show that Newton's method for $f(x) = x -a \sin x - b$ with starting value
	\begin{equation*}
	x_0 = \begin{cases}
	(m+1)\pi &\text{if }(-1)^ma >0 \\
	m\pi &\text{otherwise}
	\end{cases}
	\end{equation*}
	is guaranteed to converge (monotonically) to $x^\star(a,b)$.
\end{enumerate}
\end{problem}

\begin{problem}[Basic]
Consider the two equivalent equations
\begin{align}
x\log x -1 &= 0, \label{eq:GaustchiLog1} \\
\log x - \frac{1}{x} &= 0. \label{eq:GaustchiLog2}
\end{align}
\begin{enumerate}
	\item Show that there is exactly one positive root and find a rough interval containing it.
	\item For both \eqref{eq:GaustchiLog1} and \eqref{eq:GaustchiLog2}, determine the largest interval on which Newton's method converges. \newline \textbf{Hint: } Investigate the convexity of the functions involved.
\end{enumerate}
\end{problem}

\begin{problem}[CAS]
Design a process in \texttt{desmos.com} to test the search for critical points given by the recursion formulas produced by Newton's method.
\begin{figure}[ht!]
\includegraphics[width=\linewidth]{images/desmos.png}
\caption{Newton method in \texttt{desmos.com}}
\label{figure:desmosNewton}
\end{figure}
\end{problem}

\begin{problem}[CAS]\index{Horner's!method}\index{Horner's!scheme}
The purpose of this problem is the design of \emph{Horner's method} to evaluate polynomials effectively.  Given a polynomial
\begin{equation*}
p(x) = \sum_{k=0}^n a_k x^k = a_0 + a_1 x + \dotsb + a_n x^n,
\end{equation*}
where $a_0, a_1, \dotsc, a_n$ are real numbers, and given $x_0 \in \field{R}$, we define the Horner's scheme $\{ b_0, b_1, \dotsc, b_n \}$ to evaluate $p(x_0)$ as follows:
\begin{align*}
b_n &= a_n \\
b_{n-1} &= a_{n-1} + b_n x_0 \\
b_{n-2} &= a_{n-2} + b_{n-1} x_0 \\
&\vdots \\
b_0 &= a_0 + b_1 x_0
\end{align*}
\begin{enumerate}
	\item Prove that $b_0 = p(x_0)$
	\item Use Horner's method to evaluate $p(x) = 2x^3 - 6x^2 +2x -1$ at $x=3$.  Illustrate all steps, and count the number of basic operations (addition, subtraction, multiplication, division) used.
	\item Employ the usual method of evaluation of polynomials to evaluate $p(x) = 2x^3 - 6x^2 +2x -1$ at $x=3$.  Count the number of basic operations (note that a raising to the cube counts as two multiplications, e.g.)
	\item In a computer language or CAS of your choice, write a routine to apply Horner's scheme to evaluate polynomials.  Your routine should gather the following inputs:
	\begin{itemize}
		\item A list of coefficients \texttt{[a0, a1, ..., an]} representing the polynomial $p(x)$.
		\item A value $x_0$
	\end{itemize}
	The output of your routine should be $p(x_0)$.
\end{enumerate}
\end{problem}

\begin{problem}[CAS]\label{problem:CASNewton}
In a computer language or CAS of your choice, design a routine that gathers the following as input:
\begin{itemize}
\item the definition of a generic real-valued function $f\colon \field{R}^d \to \field{R}$, 
\item the gradient $\gradient{f}$ of that function,
\item an initial guess $\x_0 \in \field{R}^d$, 
\item a number $N$ of steps,
\end{itemize}
and produces the first $N+1$ terms of the Newton sequence to approximate a root of $f$.

Modify the previous routine to receive as input, instead of a number of steps, a \emph{tolerance} \texttt{tol} indicating the accuracy of the solution.  For example, if we require a root of the equation $f(x)=0$ accurate to the first 16 correct decimal places, we use \texttt{tol = 1e-16}.
\end{problem}

\begin{problem}[CAS]
Use any of the routines that you wrote in Problem \ref{problem:CASNewton} to produce a table and a visual representation for the numerical solution of the following equations, with the given initial guesses and steps.
\begin{enumerate}
\item $f(x) = \sin x$, with $x_0=0.5$, 5 steps.
\item $f(x) = \sin x$, with $x=3$, enough steps to obtain accurately the first 16 correct decimal places of $\pi$.
\item $f(x) = -1+\log x$, with $x=2$, enough steps to obtain accurately the first 16 correct decimal places of $e$.
\end{enumerate}
\end{problem}

\begin{problem}[CAS]
The objective of this problem is to use Newton's method to find an approximation to the golden ratio $\phi=\frac{1}{2}(1+\sqrt{5})$ accurate to the first 16 decimal places.  Find first an appropriate polynomial $p(x)$ with integer coefficients for which $\phi$ is a root.  Employ any of the routines that you wrote in Problem \ref{problem:CASNewton} with a good initial guess to guarantee the required result.
\end{problem}

\begin{problem}[Intermediate|CAS]\cite[lec3\_newton\_mthd, 3.1]{Freund2004nonlinear}
Consider the function 
\begin{equation*}
f(x) = 9x -4\log(x-7).
\end{equation*}
We wish to study the behavior of Newton-Raphson to find approximations to the critical points of this function.
\begin{enumerate}
	\item Find the domain $D$ of $f$.
	\item Find the global minimum of $f$ analytically.
	\item Compute an exact formula for the Newton-Raphson iterate $x_{n+1}$ for an initial guess $x_0 \in D$.
	\item Compute five iterations of the Newton-Raphson method starting at each of the following initial guesses:
	\begin{enumerate}
		\item $x_0 = 7.4$.
		\item $x_0 = 7.2$.
		\item $x_0 = 7.01$.
		\item $x_0 = 7.8$.
		\item $x_0 = 7.88$.
	\end{enumerate}
	\item Prove that the Newton-Raphson method converges to the optimal solution for any initial guess $x_0 \in (7,7.8888)$.
	\item What is the behavior of the Newton-Raphson method if the initial guess is not in the interval $(7,7.8888)$?
\end{enumerate}
\end{problem}

\begin{problem}[Intermediate|CAS]\cite[lec3\_newton\_mthd, 3.2]{Freund2004nonlinear}
Consider the function 
\begin{equation*}
f(x) = 6x -4\log(x-2) -3\log(25-x).
\end{equation*}
We wish to study the behavior of Newton-Raphson to find approximations to the critical points of this function.
\begin{enumerate}
	\item Find the domain $D$ of $f$.
	\item Find the global minimum of $f$ analytically. 
	\item Compute an exact formula for the Newton-Raphson iterate $x_{n+1}$ for an initial guess $x_0 \in D$.
	\item Compute five iterations of the Newton-Raphson method starting at each of the following initial guesses:
	\begin{enumerate}
		\item $x_0 = 2.6$.
		\item $x_0 = 2.7$.
		\item $x_0 = 2.4$.
		\item $x_0 = 2.8$.
		\item $x_0 = 3$.
	\end{enumerate}
	\item Prove that the Newton-Raphson method converges to the optimal solution for any initial guess $x_0 \in (2,3.05)$.
	\item What is the behavior of the Newton-Raphson method if the initial guess is not in the interval $(2,3.05)$?
\end{enumerate}
\end{problem}

\begin{problem}[Advanced]\cite[p.91 \#1.4.1]{bertsekas1999nonlinear}
The purpose of this exercise is to show that Newton's method is unaffected by linear scaling of the variables.  Consider a linear invertible transformation of variables $\x=\boldsymbol{A}\y$.  Write Newton's method in the space of the variables $\y$ and show that it generates the sequence $\y_n = \boldsymbol{A}^{-1}\x_n$, where $\{ \x_n \}_{n \in \field{N}}$ is the sequence generated by Newton's method in the space of variables $\x$.\
\end{problem}

\begin{problem}[Advanced]
Prove Theorems \ref{theorem:SteepestDescentDescends}, and \ref{theorem:SteepestDescentConvergesToCritical}.
\end{problem}

\begin{problem}[Basic]\index{LU-decomposition}
Let $\boldsymbol{A}$ be a square matrix.  An \emph{LU-decomposition} is a factorization of $\boldsymbol{A}=\boldsymbol{L} \cdot \boldsymbol{U}$ into a \emph{lower triangular} matrix $\boldsymbol{L}$ and an \emph{upper triangular} matrix $\boldsymbol{U}$, both of which have non-zero entries in their diagonals.  For example, the general case for $3 \times 3$ square matrices:
\begin{equation*}
\begin{bmatrix} 
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{bmatrix} = \underbrace{\begin{bmatrix}
\ell_{11} & 0 & 0 \\
\ell_{21} & \ell_{22} & 0 \\
\ell_{31} & \ell_{32} & \ell_{33}
\end{bmatrix}}_{\boldsymbol{L}} \cdot \underbrace{\begin{bmatrix}
u_{11} & u_{12} & u_{13} \\
0 & u_{22} & u_{23} \\
0 & 0 & u_{33}
\end{bmatrix}}_{\boldsymbol{U}}
\end{equation*}
\begin{enumerate}
	\item Find an LU-decomposition of the following matrix
	\begin{equation*}
	\begin{bmatrix} 4 & 3 \\ 6 & 3 \end{bmatrix}
	\end{equation*}
	that satisfies that all diagonal entries of $\boldsymbol{L}$ are ones.
	\item Find an example of a $2\times 2$ square matrix for which there is not any possible LU-decomposition.
\end{enumerate}
\end{problem}

\begin{problem}[Advanced]\index{Cholesky decomposition}
Prove the following statements:
\begin{enumerate}
	\item A square matrix $\boldsymbol{A}$ of size $d \times d$ admits an LU-decomposition if and only if the leading principal minors are non-zero: $\Delta_k \neq 0$ for $1 \leq k \leq d$.
	\item If $\boldsymbol{A}$ is a symmetric positive definite matrix, then it is possible to find an LU-decomposition where $\boldsymbol{U} = \transpose{\boldsymbol{L}}$: $\boldsymbol{A} = \boldsymbol{L} \cdot \transpose{\boldsymbol{L}}$.  In this case, this factorization is also called a \emph{Cholesky decomposition}.
\end{enumerate}
\end{problem}

\begin{problem}[Advanced]\index{Theorem!Kantorovich estimate}
We want to prove estimate \eqref{equation:KantorovichEstimate} in the proof of Theorem \ref{theorem:KantorovichEstimate} in page \pageref{equation:KantorovichEstimate}.  This follows directly from the equivalent statement below, which is easier to handle.  Prove the following result:
\begin{center}
\begin{tikzpicture}
\draw (0,0) node[text justified, text width=0.85\linewidth, draw, rounded corners, inner sep=10]{
\noindent\textbf{Kantorovich Estimate}. Given a positive definite symmetric matrix $Q$ of size $d \times d$, consider the quadratic function $p(\x) = \tfrac{1}{2}\quadratic{Q}(\x)$.  Assume $0 < \lambda_1 \leq \lambda_2 \leq \dotsb \leq \lambda_d$ are the eigenvalues of $Q$.  For any sequence $\{ \x_n \}_{n \in \field{N}}$ of steepest descent for $f$, we have the following estimate involving the directions of steepest descent $\{ \v_n \}_{n \in \field{N}}$:
\begin{equation*}
\frac{\norm{\v_n}^4}{\quadratic{Q}(\v_n) \quadratic{(Q^{-1})}(\v_n)} \geq \frac{4\lambda_1\lambda_d}{(\lambda_1 + \lambda_d)^2}
\end{equation*}
};
\end{tikzpicture}
\end{center}
\end{problem}

\begin{problem}\cite[lec5\_steep\_desce, 8.3]{Freund2004nonlinear}
Consider the quadratic polynomial 
\begin{equation*}
p(x,y) = \tfrac{1}{2} \quadratic{Q}(x,y) + \langle D, [x,y] \rangle + 13,
\end{equation*}
with
\begin{align*}
D &= [4, -15], & Q &= \begin{bmatrix} 10 & -9 \\ -9 & 10 \end{bmatrix}
\end{align*}
\begin{enumerate}
	\item Find the global minimum value of $p$, and its location.
	\item Compute the eigenvalues of $Q$.  Is $Q$ positive definite?
	\item What is the worst-case scenario rate of convergence of sequences of steepest descent for this function?
	\item Compute sequences of steepest descent for this function with the initial guesses below.  Make sure to report a table similar to the one in Example \ref{example:SDconvergenceRate}.
	\begin{itemize}
		\item $(0,0)$
		\item $(-0.4, 0)$
		\item $(10,0)$
		\item $(11, 0)$
	\end{itemize}
\end{enumerate}
\end{problem}

\begin{problem}\cite[lec5\_steep\_desce, 8.4]{Freund2004nonlinear}
Consider the quadratic polynomial 
\begin{equation*}
p(x,y,z) = \tfrac{1}{2} \quadratic{Q}(x,y,z) + \langle D, [x,y,z] \rangle,
\end{equation*}
with
\begin{align*}
D &= [12, -47, -8], & Q &= \begin{bmatrix} 10 & -18 & 2 \\ -18 & 40 & -1 \\ 2 & -1 & 3 \end{bmatrix}
\end{align*}
\begin{enumerate}
	\item Find the global minimum value of $p$, and its location.
	\item Compute the eigenvalues of $Q$.  Is $Q$ positive definite?
	\item What is the worst-case scenario rate of convergence of sequences of steepest descent for this function?
	\item Compute sequences of steepest descent for this function with the initial guesses below.  Make sure to report a table similar to the one in Example \ref{example:SDconvergenceRate}.
	\begin{itemize}
		\item $(0,0,0)$
		\item $(15.09, 7.66, -6.56)$
		\item $(11.77, 6.42, -4.28)$
		\item $(4.46, 2.25, 1.85)$
	\end{itemize}
\end{enumerate}
\end{problem}